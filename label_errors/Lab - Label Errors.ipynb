{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwfWqPeA1zX0"
   },
   "source": [
    "# Lab â€” Label Errors\n",
    "\n",
    "This lab highlights data-centric AI techniques (using [confident learning](https://jair.org/index.php/jair/article/view/12125)) to improve the accuracy of an XGBoost classifier on a noisy dataset that has label errors.\n",
    "\n",
    "The DCAI techniques demonstrated in this lab involve optimizing the dataset itself rather than altering the model's architecture or hyperparameters. As a result, it is possible to achieve further improvements in accuracy by fine-tuning the model in conjunction with the newly enhanced data, but that is not the focus of this lab.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- Establish a baseline [XGBoost](https://xgboost.readthedocs.io/) model accuracy on the original data\n",
    "- Automatically find mislabeled data points by:\n",
    "    - Computing out-of-sample predicted probabilities\n",
    "    - Estimating the number of label errors using confident learning\n",
    "    - Ranking errors, using the number of label errors as a cutoff in identifying issues\n",
    "- Remove the bad data\n",
    "- Retrain the exact same XGBoost model to see the improvement in test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software installation\n",
    "\n",
    "This lab relies on a couple PyPI packages. If you don't have them installed, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7 in /usr/local/python/3.10.4/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: cleanlab in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from xgboost==1.7) (1.24.2)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from xgboost==1.7) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: termcolor>=2.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from cleanlab) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.53.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from cleanlab) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7 scikit-learn pandas cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je7P55z4RwX_"
   },
   "source": [
    "## Setup and Data Processing\n",
    "\n",
    "Let's take a look at the dataset used in this lab, a tabular dataset of student grades.\n",
    "\n",
    "The data includes three exam scores (numerical features), a written note (categorical feature with missing values), and a (noisy) letter grade (categorical label). Our aim is to train a model to classify the grade for each student based on the other features.\n",
    "\n",
    "In this dataset, 20% of the grade labels are actually incorrect (the `noisy_letter_grade` column). Synthetic noise was added to this dataset for the purpose of this lab. In this lab, we have access to the true letter grade each student should have received (the `letter_grade` column), which we use for evaluating both the underlying accuracy of model predictions and how well our approach detects which data are mislabeled. We are careful to only use these true grades for evaluation, not for model training.\n",
    "\n",
    "In the real world, you don't have access to the true labels (you only observe the `noisy_letter_grade`, not the true `letter_grade`). So when evaluating models in the real world, you have to be careful to make sure that your test set is free of error (using methods like those covered in this lab, ideally combined with human review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQVmMBQOS43j",
    "outputId": "54a45659-ecb6-47fe-acfa-d0424027af47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stud_ID': '4a6f46', 'exam_1': 76, 'exam_2': 82, 'exam_3': 91, 'notes': 'great final presentation +10', 'letter_grade': 'A', 'noisy_letter_grade': 'A'}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./student-grades.csv\")\n",
    "df.head()\n",
    "print(df.iloc[630].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3 notes  letter_grade  noisy_letter_grade\n",
       "0  f48f73      53      77      93     5             2                   2\n",
       "1  0bd4e7      81      64      80     2             1                   1\n",
       "2  e1795d      74      88      97     5             1                   1\n",
       "3  cb9d7a      61      94      78     5             2                   2\n",
       "4  9acca4      48      90      91     5             2                   2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = df.copy()\n",
    "# Transform letter grades and notes to categorical numbers.\n",
    "# Necessary for XGBoost.\n",
    "df['letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['letter_grade'])\n",
    "df['noisy_letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['noisy_letter_grade'])\n",
    "df['notes'] = preprocessing.LabelEncoder().fit_transform(df[\"notes\"])\n",
    "df['notes'] = df['notes'].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVH_iciASD9F"
   },
   "source": [
    "# Get What We Need\n",
    "\n",
    "To apply confident learning (the technique explained in today's lecture), we need to obtain [**out-of-sample** predicted probabilities](https://docs.cleanlab.ai/stable/tutorials/pred_probs_cross_val.html#out-of-sample-predicted-probabilities) for all of our data. To do this, we can use K-fold cross validation: for each fold, we will train on some subset of our data and get predictions on the rest of the data that was _not_ used for training.\n",
    "\n",
    "We need to choose a model in order to do this. For this lab, we'll use [XGBoost](https://xgboost.readthedocs.io/), a library implementing gradient-boosted decision trees, a class of model commonly used for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCS19IqJsQUL",
    "outputId": "fd89b945-6793-4b3a-a017-7b19f8e6a29b"
   },
   "outputs": [],
   "source": [
    "# Prepare training data (remove labels from the dataframe) and labels\n",
    "data = df.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "labels = df['noisy_letter_grade']\n",
    "\n",
    "# XGBoost(experimental) supports categorical data.\n",
    "# Here we use default hyperparameters for simplicity.\n",
    "# Get out-of-sample predicted probabilities and check model accuracy.\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: getting out-of-sample predicted probabilities\n",
    "\n",
    "Compute out-of-sample predicted probabilities for every data point. You can do this manually using for loops and multiple invocations of model training and prediction, or you can use scikit-learn's [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) (if you're using this function, take a look at the documentations, and in particular, the `method=` keyword argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_probs should be a Nx5 matrix of out-of-sample predicted probabilities, with N = len(data)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pred_probs = cross_val_predict(model, data, labels, method='predict_proba')\n",
    "\n",
    "# def split_into_parts(arr, no_of_splits=3):\n",
    "#     parts = []\n",
    "#     len_of_part = math.ceil(len(arr)/no_of_splits)\n",
    "#     for i in range(no_of_splits):\n",
    "#         parts.append(arr[i*len_of_part:(i+1)*len_of_part])\n",
    "#     return parts\n",
    "\n",
    "# def train_val_split(data, labels):\n",
    "\n",
    "#     train_val_splits = []\n",
    "\n",
    "#     data_parts = split_into_parts(data)\n",
    "#     label_parts = split_into_parts(labels)\n",
    "\n",
    "#     train_val_splits.append((pd.concat([data_parts[0],data_parts[1]]), pd.concat([label_parts[0],label_parts[1]]), data_parts[2], label_parts[2]))\n",
    "#     train_val_splits.append((pd.concat([data_parts[1],data_parts[2]]), pd.concat([label_parts[1],label_parts[2]]), data_parts[0], label_parts[0]))\n",
    "#     train_val_splits.append((pd.concat([data_parts[2],data_parts[0]]), pd.concat([label_parts[2],label_parts[0]]), data_parts[1], label_parts[1]))\n",
    "\n",
    "#     return train_val_splits\n",
    "\n",
    "# # pred_probs should be a Nx5 matrix of out-of-sample predicted probabilities, with N = len(data)\n",
    "# pred_probs = []\n",
    "# y_labels = []\n",
    "# splits = train_val_split(data, labels)\n",
    "# for i in range(3):\n",
    "#     X_train, y_train, X_test, y_test = splits[i]\n",
    "#     model.fit(X_train, y_train)\n",
    "#     pred_probs.append(model.predict_proba(X_test))\n",
    "\n",
    "# pred_probs = np.concatenate(pred_probs, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model accuracy on original data\n",
    "\n",
    "Now that we have out-of-sample predicted probabilities, we can also check the model's (cross-val) accuracy on the original (noisy) data, so we'll have a baseline to compare our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 67.4%\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(pred_probs, axis=1)\n",
    "acc_original = accuracy_score(preds, labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding label issues automatically\n",
    "\n",
    "We count label issues using confident learning. First, we need to compute class thresholds for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: computing class thresholds\n",
    "\n",
    "Implement the Confident Learning algorithm for computing class thresholds for the 5 classes. You can refer to slide 26 from today's lecture or see equation 2 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The class threshold for each class is the model's expected (average) self-confidence for each class. In other words, to compute the threshold for a particular class, you can average the predicted probability for that class, for all datapoints that are labeled with that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    count_per_class = np.unique(labels, return_counts=True)[1]\n",
    "    probs_sum_per_class = []\n",
    "    for i in range(len(count_per_class)):\n",
    "        probs_sum_per_class.append(pred_probs[labels==i, i].sum())\n",
    "    return np.array([probs_sum_per_class[i]/count_per_class[i] for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be a numpy array of length 5\n",
    "thresholds = compute_class_thresholds(pred_probs, labels.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: constructing the confident joint\n",
    "\n",
    "Next, we compute the confident joint, a matrix that counts the number of label errors for each noisy label $\\tilde{y}$ and true label $y^*$. You can follow the algorithm that we walked through in slide 27 from today's lecture, or see equation 1 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The confident joint C is a K x K matrix (with K = 5 for this dataset), where `C[i][j]` is an estimate of the count of the number of data points with noisy label `i` and true label `j`. From lecture, recall that we put a data point in bin `(i, j)` if its given label is `i`, and its predicted probability for class `j` is above the threshold for class `j` (`thresholds[j]`). Each data point should only go in a single bin; if a data point's predicted probability is above the class threshold for multiple classes, it goes in the bin for which it has the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint(pred_probs: np.ndarray, labels: np.ndarray, thresholds: np.ndarray) -> np.ndarray:\n",
    "    C_ij = np.zeros((pred_probs.shape[1], pred_probs.shape[1]))\n",
    "\n",
    "    for i in range(pred_probs.shape[0]):\n",
    "        for j in range(pred_probs.shape[1]):\n",
    "            if pred_probs[i,j] >= thresholds[j]:\n",
    "                C_ij[j, labels[i]] += 1\n",
    "\n",
    "    return C_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = compute_confident_joint(pred_probs, labels.to_numpy(), thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.0\n"
     ]
    }
   ],
   "source": [
    "num_label_issues = 0\n",
    "for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            if i != j:\n",
    "                  num_label_issues += C[i][j]\n",
    "\n",
    "print(num_label_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated noise rate: 24.4%\n"
     ]
    }
   ],
   "source": [
    "print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / pred_probs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: filter out label issues\n",
    "\n",
    "In this lab, our approach to identifying issues is to rank the data points by a score (\"self-confidence\", the model's predicted probability for a data point's given label) and then take the top `num_label_issues` of those.\n",
    "\n",
    "First, we want to compute the model's _self-confidence_ for each data point. For a data point `i`, that is `pred_probs[i, labels[i]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a numpy array of length 941 of probabilities\n",
    "self_confidences = []\n",
    "\n",
    "for i in range(pred_probs.shape[0]):\n",
    "    self_confidences.append(pred_probs[i, labels[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we rank the _indices_ of the data points by the self-confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a numpy array of length 941 of integer indices\n",
    "ranked_indices = np.argsort(self_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the indices of label issues as the top `num_label_issues` items in the `ranked_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_idx = ranked_indices[:int(num_label_issues)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of the highest-ranked data points (most likely to be label issues):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvJHkPzSq6Q"
   },
   "source": [
    "# How'd We Do?\n",
    "\n",
    "Let's go a step further and see how we did at automatically identifying which data points are mislabeled. If we take the intersection of the labels errors identified by Confident Learning and the true label errors, we see that our approach was able to identify 75% of the label errors correctly (based on predictions from a model that is only 67% accurate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>77c9c5</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>dacfb9</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>e2614a</td>\n",
       "      <td>85</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b0306d</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>347d55</td>\n",
       "      <td>98</td>\n",
       "      <td>51</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stud_ID  exam_1  exam_2  exam_3                       notes letter_grade  \\\n",
       "637  77c9c5       0      79      65  cheated on exam, gets 0pts            F   \n",
       "264  dacfb9      80      60      80                         NaN            C   \n",
       "378  e2614a      85      62      75                         NaN            C   \n",
       "689  b0306d      77      51      70                         NaN            D   \n",
       "318  347d55      98      51      74                         NaN            C   \n",
       "\n",
       "    noisy_letter_grade  \n",
       "637                  A  \n",
       "264                  F  \n",
       "378                  F  \n",
       "689                  B  \n",
       "318                  F  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.iloc[ranked_indices[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O2a6urWc1DA",
    "outputId": "f88b5ce6-33f2-4ef6-e774-19013c33f0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of errors found: 76.6%\n"
     ]
    }
   ],
   "source": [
    "# Computing percentage of true errors identified. \n",
    "true_error_idx = df[df.letter_grade != df.noisy_letter_grade].index.values\n",
    "cl_acc = len(set(true_error_idx).intersection(set(issue_idx)))/len(true_error_idx)\n",
    "print(f\"Percentage of errors found: {round(cl_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzxXoDOqSzn-"
   },
   "source": [
    "# Train a More Robust Model\n",
    "\n",
    "Now that we have the indices of potential label errors within our data, let's remove them from our data, retrain our model, and see what improvement we can gain.\n",
    "\n",
    "Keep in mind that our baseline model from above, trained on the original data using the `noisy_letter_grade` as the prediction label, achieved a cross-validation accuracy of 67%.\n",
    "\n",
    "Let's use a very simple method to handle these label errors and just drop them entirely from the data and retrain our exact same `XGBClassifier`. In a real-world application, a better approach might be to have humans review the issues and _correct_ the labels rather than dropping the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsQFmy7xgSUa",
    "outputId": "a33e17ab-c197-4f95-c9c1-0473c16af313"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[637, 264, 378, 689, 318, 886, 36, 83, 709, 937, 173, 404, 539, 23, 159, 301, 71, 885, 498, 864, 795, 680, 476, 765, 800, 65, 760, 40, 11, 410, 744, 421, 263, 457, 872, 673, 502, 910, 139, 169, 660, 619, 144, 27, 18, 629, 936, 285, 552, 918, 129, 827, 723, 666, 491, 179, 208, 292, 69, 653, 550, 184, 230, 836, 892, 324, 259, 332, 224, 255, 840, 931, 607, 354, 367, 811, 484, 437, 322, 787, 56, 409, 238, 562, 337, 158, 130, 828, 817, 928, 814, 174, 199, 14, 854, 595, 440, 373, 641, 549, 198, 73, 548, 225, 882, 705, 815, 504, 284, 253, 545, 143, 567, 579, 430, 755, 161, 210, 62, 911, 120, 272, 763, 456, 597, 853, 862, 938, 383, 105, 816, 57, 826, 631, 274, 638, 45, 382, 496, 478, 774, 303, 309, 219, 468, 721, 137, 725, 452, 773, 325, 561, 138, 186, 824, 438, 553, 733, 865, 17, 312, 451, 290, 893, 642, 343, 197, 648, 333, 791, 517, 682, 122, 431, 799, 790, 900, 58, 55, 934, 12, 260, 764, 271, 49, 762, 727, 940, 877, 506, 195, 406, 221, 560, 584, 206, 766, 926, 329, 781, 392, 751, 481, 32, 847, 31, 651, 813, 400, 662, 391, 160, 37, 190, 459, 923, 140, 149, 557, 538, 841, 441, 108, 470, 524, 217, 521, 212, 887, 233] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Remove the label errors found by Confident Learning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mdrop(issue_idx)\n\u001b[1;32m      3\u001b[0m clean_labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mdrop(issue_idx)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Train a more robust classifier with less erroneous data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5407\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[637, 264, 378, 689, 318, 886, 36, 83, 709, 937, 173, 404, 539, 23, 159, 301, 71, 885, 498, 864, 795, 680, 476, 765, 800, 65, 760, 40, 11, 410, 744, 421, 263, 457, 872, 673, 502, 910, 139, 169, 660, 619, 144, 27, 18, 629, 936, 285, 552, 918, 129, 827, 723, 666, 491, 179, 208, 292, 69, 653, 550, 184, 230, 836, 892, 324, 259, 332, 224, 255, 840, 931, 607, 354, 367, 811, 484, 437, 322, 787, 56, 409, 238, 562, 337, 158, 130, 828, 817, 928, 814, 174, 199, 14, 854, 595, 440, 373, 641, 549, 198, 73, 548, 225, 882, 705, 815, 504, 284, 253, 545, 143, 567, 579, 430, 755, 161, 210, 62, 911, 120, 272, 763, 456, 597, 853, 862, 938, 383, 105, 816, 57, 826, 631, 274, 638, 45, 382, 496, 478, 774, 303, 309, 219, 468, 721, 137, 725, 452, 773, 325, 561, 138, 186, 824, 438, 553, 733, 865, 17, 312, 451, 290, 893, 642, 343, 197, 648, 333, 791, 517, 682, 122, 431, 799, 790, 900, 58, 55, 934, 12, 260, 764, 271, 49, 762, 727, 940, 877, 506, 195, 406, 221, 560, 584, 206, 766, 926, 329, 781, 392, 751, 481, 32, 847, 31, 651, 813, 400, 662, 391, 160, 37, 190, 459, 923, 140, 149, 557, 538, 841, 441, 108, 470, 524, 217, 521, 212, 887, 233] not found in axis'"
     ]
    }
   ],
   "source": [
    "# Remove the label errors found by Confident Learning\n",
    "data = data.drop(issue_idx)\n",
    "clean_labels = labels.drop(issue_idx)\n",
    "\n",
    "# Train a more robust classifier with less erroneous data\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "clean_pred_probs = cross_val_predict(model, data, clean_labels, method='predict_proba')\n",
    "clean_preds = np.argmax(clean_pred_probs, axis=1)\n",
    "\n",
    "acc_clean = accuracy_score(clean_preds, clean_labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100, 1)}%\")\n",
    "print(f\"Accuracy with errors found by Confident Learning removed: {round(acc_clean*100, 1)}%\")\n",
    "\n",
    "# Compute reduction in error.\n",
    "err = ((1-acc_original)-(1-acc_clean))/(1-acc_original)\n",
    "print(f\"Reduction in error: {round(err*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J9clVf1UzQZ"
   },
   "source": [
    "After removing the suspected label issues, our model's new cross-validation accuracy is now 90%, which means we **reduced the error-rate of the model by 70%** (the original model had 67% accuracy). \n",
    "\n",
    "**Note: throughout this entire process we never changed any code related to model architecture/hyperparameters, training, or data preprocessing!  This improvement is strictly coming from increasing the quality of our data which leaves additional room for additional optimizations on the modeling side.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W-Lo82SVp7I"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "For the student grades dataset, we found that simply dropping identified label errors and retraining the model resulted in a 70% reduction in prediction error on our classification problem (with accuracy improving from 67% to 90%).\n",
    "\n",
    "An implementation of the Confident Learning algorithm (and much more) is available in the [cleanlab](https://github.com/cleanlab/cleanlab) library on GitHub. This is how today's lab assignment can be done in a single line of code with Cleanlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "\n",
    "cl_issue_idx = cleanlab.filter.find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.iloc[cl_issue_idx[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Advanced topic_: you might notice that the above `cl_issue_idx` differs in length (by a little bit) from our `issue_idx`. The reason for this is that we implemented a slightly simplified version of the algorithm in this lab. We skipped a calibration step after computing the confident joint that makes the confident joint have the true noisy prior $p(labels)$ (summed over columns for each row) and also add up to the total number of examples. If you're interested in the details of this, see equation 3 and the subsequent explanation in the [paper](https://jair.org/index.php/jair/article/view/12125)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
